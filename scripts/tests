#!/usr/bin/env python
# SPDX-License-Identifier: Apache-2.0

import platform
import sys
import os
import subprocess
import logging
import click
import hashlib
import asyncio
from functools import reduce
from enum import IntEnum
from typing import Optional, Callable, TypedDict


def config_logger(verbose):
    if verbose:
        logging.basicConfig(
            stream=sys.stdout,
            format="%(levelname)-5s > %(message)s",
            level=logging.DEBUG,
        )
    else:
        logging.basicConfig(
            stream=sys.stdout,
            format="%(levelname)-5s > %(message)s",
            level=logging.INFO,
        )


def sha256sum(result: bytes) -> str:
    m = hashlib.sha256()
    m.update(result)
    return m.hexdigest()


class TEST_TYPES(IntEnum):
    MLKEM = 1
    BENCH = 2
    NISTKAT = 3
    KAT = 4
    BENCH_COMPONENTS = 5

    def __str__(self):
        return self.name.lower()

    def desc(self):
        match self:
            case TEST_TYPES.MLKEM:
                return "Functional Test"
            case TEST_TYPES.BENCH:
                return "Benchmark"
            case TEST_TYPES.BENCH_COMPONENTS:
                return "Benchmark Components"
            case TEST_TYPES.NISTKAT:
                return "Nistkat Test"
            case TEST_TYPES.KAT:
                return "Kat Test"

    def bin(self):
        match self:
            case TEST_TYPES.MLKEM:
                return "test_kyber"
            case TEST_TYPES.BENCH:
                return "bench_kyber"
            case TEST_TYPES.BENCH_COMPONENTS:
                return "bench_components_kyber"
            case TEST_TYPES.NISTKAT:
                return "gen_NISTKAT"
            case TEST_TYPES.KAT:
                return "gen_KAT"

    def bin_path(self, scheme):
        return f"test/build/{scheme.name.lower()}/bin/{self.bin()}{scheme.suffix()}"


class SCHEME(IntEnum):
    MLKEM512 = 1
    MLKEM768 = 2
    MLKEM1024 = 3

    def __str__(self):
        match self:
            case SCHEME.MLKEM512:
                return "ML-KEM-512"
            case SCHEME.MLKEM768:
                return "ML-KEM-768"
            case SCHEME.MLKEM1024:
                return "ML-KEM-1024"

    def __iter__(self):
        return self

    def __next__(self):
        return self + 1

    def suffix(self):
        return self.name.removeprefix("MLKEM")


def parse_meta(scheme: SCHEME, field: str) -> str:
    result = subprocess.run(
        [
            "yq",
            "-r",
            "--arg",
            "scheme",
            str(scheme),
            f'.implementations.[] | select(.name == $scheme) | ."{field}"',
            "./META.yml",
        ],
        capture_output=True,
        encoding="utf-8",
        universal_newlines=False,
    )
    return result.stdout.strip()


def github_summary(title: str, test: TEST_TYPES, results: TypedDict):
    summary_file = os.environ.get("GITHUB_STEP_SUMMARY")

    res = list(results.values())

    if isinstance(results[SCHEME.MLKEM512], str):
        summaries = list(
            map(
                lambda s: f" {s} |",
                reduce(
                    lambda acc, s: [
                        line1 + " | " + line2 for line1, line2 in zip(acc, s)
                    ],
                    [s.splitlines() for s in res],
                ),
            )
        )
        summaries = [f"| {test.desc()} |" + summaries[0]] + [
            "| |" + x for x in summaries[1:]
        ]
    else:
        summaries = [
            reduce(
                lambda acc, b: f"{acc} " + (":x: |" if b else ":white_check_mark: |"),
                res,
                f"| {test.desc()} |",
            )
        ]

    def find_last_consecutive_match(l, s):
        for i, v in enumerate(l[s + 1 :]):
            if not v.startswith("|") or not v.endswith("|"):
                return i + 1
        return len(l)

    def add_summaries(fn, title, summaries):
        summary_title = "| Tests |"
        summary_table_format = "| ----- |"
        for s in SCHEME:
            summary_title += f" {s} |"
            summary_table_format += " ----- |"

        with open(fn, "r") as f:
            pre_summaries = [x for x in f.read().splitlines() if x]
            if title in pre_summaries:
                if summary_title not in pre_summaries:
                    summaries = [summary_title, summary_table_format] + summaries
                    pre_summaries = (
                        pre_summaries[: pre_summaries.index(title) + 1]
                        + summaries
                        + pre_summaries[pre_summaries.index(title) + 1 :]
                    )
                else:
                    i = find_last_consecutive_match(
                        pre_summaries, pre_summaries.index(title)
                    )
                    pre_summaries = pre_summaries[:i] + summaries + pre_summaries[i:]
                return ("w", pre_summaries)
            else:
                pre_summaries = [
                    title,
                    summary_title,
                    summary_table_format,
                ] + summaries
                return ("a", pre_summaries)

    if summary_file is not None:
        (access_mode, summaries) = add_summaries(summary_file, title, summaries)
        with open(summary_file, access_mode) as f:
            print("\n".join(summaries), file=f)


class State(object):

    def __init__(self):
        self.verbose = False
        self.cross_prefix = ""
        self.cflags = ""
        self.arch_flags = ""
        self.opt = True
        self.auto = True
        self.compile = True
        self.run = True

    def compile_schemes(
        self,
        test_type: TEST_TYPES,
        extra_make_envs={},
        extra_make_args=[],
    ):
        """compile or cross compile with some extra environment variables and makefile arguments"""

        def dict2str(dict):
            s = ""
            for k, v in dict.items():
                s += f"{k}={v} "
            return s

        logging.debug(f"Compiling {test_type}...")
        args = [
            "make",
            f"CROSS_PREFIX={self.cross_prefix}",
            f"{test_type}",
        ] + extra_make_args

        logging.info(dict2str(extra_make_envs) + " ".join(args))

        p = subprocess.run(
            args,
            stdout=subprocess.DEVNULL if not self.verbose else None,
            env=os.environ.copy() | extra_make_envs,
        )

        if p.returncode != 0:
            logging.error(f"make failed: {p.returncode}")
            sys.exit(1)

    def run_scheme(
        self,
        bin: str,
        run_as_root=False,
        exec_wrapper=None,
    ) -> bytes:
        """Run the binary in all different ways"""
        if not os.path.isfile(bin):
            logging.error(f"{bin} does not exists")
            sys.exit(1)

        cmd = [f"./{bin}"]
        if self.cross_prefix and platform.system() != "Darwin":
            logging.info(f"Emulating {bin} with QEMU")
            if "x86_64" in self.cross_prefix:
                cmd = ["qemu-x86_64"] + cmd
            elif "aarch64" in self.cross_prefix:
                cmd = ["qemu-aarch64"] + cmd
            else:
                logging.info(
                    f"Emulation for {self.cross_prefix} on {platform.system()} not supported"
                )

        if run_as_root:
            logging.info(
                f"Running {bin} as root -- you may need to enter your root password."
            )
            cmd = ["sudo"] + cmd

        if exec_wrapper:
            logging.info(f"Running {bin} with customized wrapper.")
            exec_wrapper = exec_wrapper.split(" ")
            cmd = exec_wrapper + cmd

        logging.info(" ".join(cmd))
        result = subprocess.run(
            cmd,
            capture_output=True,
            universal_newlines=False,
        )

        if result.returncode != 0:
            logging.error(
                f"Running '{cmd}' failed: {result.returncode} {result.stderr.decode()}"
            )
            sys.exit(1)

        return result.stdout

    def run_schemes(
        self,
        test_type: TEST_TYPES,
        run_as_root=False,
        exec_wrapper=None,
        actual_proc: Callable[[bytes], str] = None,
        expect_proc: Callable[[SCHEME], str] = None,
    ) -> TypedDict:
        fail = False
        results = {}
        for scheme in SCHEME:
            result = self.run_scheme(
                test_type.bin_path(scheme),
                run_as_root,
                exec_wrapper,
            )

            if actual_proc is not None and expect_proc is not None:
                actual = actual_proc(result)
                expect = expect_proc(scheme)
                f = actual != expect
                fail = fail or f
                if f:
                    logging.error(
                        f"{scheme} failed, expecting {expect}, but getting {actual}"
                    )
                else:
                    logging.info(f"{scheme} passed")
                results[scheme] = f
            else:
                logging.info(f"{scheme}")
                logging.info(f"\n{result.decode()}")
                results[scheme] = result.decode()

        if fail:
            sys.exit(1)

        return results

    def test(
        self,
        test_type: TEST_TYPES,
        extra_make_envs={},
        extra_make_args=[],
        actual_proc: Callable[[bytes], str] = None,
        expect_proc: Callable[[SCHEME], str] = None,
        run_as_root: bool = False,
        exec_wrapper: str = None,
    ):
        config_logger(self.verbose)

        logging.info(f"{test_type.desc()}")

        make_envs = (
            {"CFLAGS": f"{self.cflags}"} if self.cflags is not None else {}
        ) | (
            {"ARCH_FLAGS": f"{self.arch_flags}"} if self.arch_flags is not None else {}
        )
        make_envs.update(extra_make_envs)

        if self.compile:
            self.compile_schemes(
                test_type,
                make_envs,
                extra_make_args
                + list(
                    set([f"OPT={int(self.opt)}", f"AUTO={int(self.auto)}"])
                    - set(extra_make_args)
                ),
            )

        results = None
        if self.run:
            results = self.run_schemes(
                test_type,
                run_as_root,
                exec_wrapper,
                actual_proc,
                expect_proc,
            )

            title = (
                "## "
                + ("Cross" if self.cross_prefix else "Native")
                + " "
                + ("Opt" if self.opt else "Non-opt")
                + " Tests"
            )
            github_summary(title, test_type, results)

        return results


def __callback(n):
    def callback(ctx, param, value):
        state = ctx.ensure_object(State)
        state.__dict__[n] = value
        return value

    return callback


_shared_options = [
    click.option(
        "-v",
        "--verbose",
        expose_value=False,
        is_flag=True,
        show_default=True,
        default=False,
        type=bool,
        help="Show verbose output or not",
        callback=__callback("verbose"),
    ),
    click.option(
        "-cp",
        "--cross-prefix",
        expose_value=False,
        default="",
        show_default=True,
        nargs=1,
        help="Cross prefix for compilation",
        callback=__callback("cross_prefix"),
    ),
    click.option(
        "--cflags",
        expose_value=False,
        nargs=1,
        help="Extra cflags to passed in (e.g. '-mcpu=cortex-a72')",
        callback=__callback("cflags"),
    ),
    click.option(
        "--arch-flags",
        expose_value=False,
        nargs=1,
        help="Extra arch flags to passed in (e.g. '-march=armv8')",
        callback=__callback("arch_flags"),
    ),
    click.option(
        "--opt/--no-opt",
        expose_value=False,
        is_flag=True,
        show_default=True,
        default=True,
        help="Choose whether to enable assembly optimizations (if present)",
        callback=__callback("opt"),
    ),
    click.option(
        "--auto/--no-auto",
        expose_value=False,
        is_flag=True,
        show_default=True,
        default=True,
        help="Allow makefile to auto configure system specific preprocessor",
        callback=__callback("auto"),
    ),
    click.option(
        "--compile/--no-compile",
        expose_value=False,
        is_flag=True,
        show_default=True,
        default=True,
        help="Determine to compile the binary or not",
        callback=__callback("compile"),
    ),
    click.option(
        "--run/--no-run",
        expose_value=False,
        is_flag=True,
        show_default=True,
        default=True,
        help="Determine to run the compiled binary or not",
        callback=__callback("run"),
    ),
]

_bench_options = [
    click.option(
        "-c",
        "--cycles",
        nargs=1,
        type=click.Choice(["NO", "PMU", "PERF", "M1"]),
        show_default=True,
        default="NO",
        help="Method for counting clock cycles. PMU requires (user-space) access to the Arm Performance Monitor Unit (PMU). PERF requires a kernel with perf support. M1 only works on Apple silicon.",
    ),
    click.option(
        "-o",
        "--output",
        nargs=1,
        help="Path to output file in json format",
    ),
    click.option(
        "-r",
        "--run-as-root",
        is_flag=True,
        show_default=True,
        default=False,
        type=bool,
        help="Benchmarking binary is run with sudo.",
    ),
    click.option(
        "-w",
        "--exec-wrapper",
        help="Run the benchmark binary with the user-customized wrapper.",
    ),
    click.option(
        "-t",
        "--mac-taskpolicy",
        nargs=1,
        type=click.Choice(["utility", "background", "maintenance"]),
        hidden=platform.system() != "Darwin",
        show_default=True,
        default=None,
        help="Run the program using the specified QoS clamp. Applies to MacOS only. Setting this flag to 'background' guarantees running on E-cores. This is an abbreviation of --exec-wrapper 'taskpolicy -c {mac_taskpolicy}'.",
    ),
    click.option(
        "--components",
        is_flag=True,
        type=bool,
        show_default=True,
        default=False,
        help="Benchmark low-level components",
    ),
]


def add_options(options):
    return lambda func: reduce(lambda f, o: o(f), reversed(options), func)


@click.group(invoke_without_command=True)
def cli():
    pass


@cli.command(
    short_help="Run the functional tests for all parameter sets",
    context_settings={"show_default": True},
)
@add_options(_shared_options)
@click.make_pass_decorator(State, ensure=True)
def func(state: State):
    def expect(scheme: SCHEME) -> str:
        sk_bytes = parse_meta(scheme, "length-secret-key")
        pk_bytes = parse_meta(scheme, "length-public-key")
        ct_bytes = parse_meta(scheme, "length-ciphertext")

        return (
            f"CRYPTO_SECRETKEYBYTES:  {sk_bytes}\n"
            + f"CRYPTO_PUBLICKEYBYTES:  {pk_bytes}\n"
            + f"CRYPTO_CIPHERTEXTBYTES: {ct_bytes}\n"
        )

    state.test(
        TEST_TYPES.MLKEM,
        actual_proc=lambda result: str(result, encoding="utf-8"),
        expect_proc=expect,
    )


@cli.command(
    short_help="Run the nistkat tests for all parameter sets",
    context_settings={"show_default": True},
)
@add_options(_shared_options)
@click.make_pass_decorator(State, ensure=True)
def nistkat(state: State):
    state.test(
        TEST_TYPES.NISTKAT,
        actual_proc=sha256sum,
        expect_proc=lambda scheme: parse_meta(scheme, "nistkat-sha256"),
    )


@cli.command(
    short_help="Run the kat tests for all parameter sets",
    context_settings={"show_default": True},
)
@add_options(_shared_options)
@click.make_pass_decorator(State, ensure=True)
def kat(state: State):
    state.test(
        TEST_TYPES.KAT,
        actual_proc=sha256sum,
        expect_proc=lambda scheme: parse_meta(scheme, "kat-sha256"),
    )


@cli.command(
    short_help="Run the benchmarks for all parameter sets",
    context_settings={"show_default": True},
)
@add_options(_shared_options)
@add_options(_bench_options)
@click.make_pass_decorator(State, ensure=True)
def bench(
    state: State,
    cycles: str,
    output,
    run_as_root: bool,
    exec_wrapper: str,
    mac_taskpolicy,
    components,
):
    config_logger(state.verbose)

    if components is False:
        bench_type = TEST_TYPES.BENCH
    else:
        bench_type = TEST_TYPES.BENCH_COMPONENTS
        output = False

    if mac_taskpolicy:
        if exec_wrapper:
            logging.error(f"cannot set both --mac-taskpolicy and --exec-wrapper")
            sys.exit(1)
        else:
            exec_wrapper = f"taskpolicy -c {mac_taskpolicy}"

    results = state.test(
        bench_type,
        extra_make_args=[f"CYCLES={cycles}"],
        run_as_root=run_as_root,
        exec_wrapper=exec_wrapper,
    )

    if results is not None and output is not None and components is False:
        import json

        with open(output, "w") as f:
            v = []
            for scheme in results:
                schemeStr = str(scheme)
                r = results[scheme]

                # The first 3 lines of the output are expected to be
                # keypair cycles=X
                # encaps cycles=X
                # decaps cycles=X

                lines = [line for line in r.splitlines() if "=" in line]

                d = {k: int(v) for k, v in (l.split("=") for l in lines)}
                for primitive in ["keypair", "encaps", "decaps"]:
                    v.append(
                        {
                            "name": f"{schemeStr} {primitive}",
                            "unit": "cycles",
                            "value": d[f"{primitive} cycles"],
                        }
                    )
            f.write(json.dumps(v))


@cli.command(
    short_help="Run all tests (except benchmark for now)",
    context_settings={"show_default": True},
)
@add_options(_shared_options)
@add_options(
    [
        click.option(
            "--func/--no-func",
            is_flag=True,
            show_default=True,
            default=True,
            help="Determine whether to run func test or not",
        ),
        click.option(
            "--kat/--no-kat",
            is_flag=True,
            show_default=True,
            default=True,
            help="Determine whether to run kat test or not",
        ),
        click.option(
            "--nistkat/--no-nistkat",
            is_flag=True,
            show_default=True,
            default=True,
            help="Determine whether to run nistkat test or not",
        ),
    ]
)
@click.make_pass_decorator(State, ensure=True)
def all(
    state: State,
    func: bool,
    kat: bool,
    nistkat: bool,
):
    opt_flag = "opt" if state.opt else "no-opt"
    auto_flag = "auto" if state.auto else "no-auto"
    compile_mode = "cross" if state.cross_prefix else "native"

    def _cmd(cmd: str, compile_flag: str, run_flag: str) -> [str]:
        return [
            "tests",
            f"{cmd}",
            f"--cross-prefix={state.cross_prefix}",
            *([f"--cflags={state.cflags}"] if state.cflags is not None else []),
            f"--{opt_flag}",
            f"--{auto_flag}",
            f"--{compile_flag}",
            f"--{run_flag}",
            *(["--verbose"] if state.verbose else []),
        ]

    def _cmds(compile_flag: str, run_flag: str) -> TypedDict:
        return {
            **({"func": _cmd("func", compile_flag, run_flag)} if func else {}),
            **({"kat": _cmd("kat", compile_flag, run_flag)} if kat else {}),
            **({"nistkat": _cmd("nistkat", compile_flag, run_flag)} if nistkat else {}),
        }

    compile_cmds = {} if not state.compile else _cmds("compile", "no-run")
    run_cmds = {} if not state.run else _cmds("no-compile", "run")

    exit_code = 0

    gh_env = os.environ.get("GITHUB_ENV")

    # sequentially compile
    for k, c in compile_cmds.items():
        if gh_env is not None:
            print(f"::group::compile {compile_mode} {opt_flag} {k} test")
        result = subprocess.run(
            c,
            encoding="utf-8",
            capture_output=True,
            universal_newlines=False,
        )
        exit_code = exit_code or result.returncode
        print(result.stdout)
        if result.returncode != 0:
            print(f"{result.stderr}")
            run_cmds.pop(k, None)
        if gh_env is not None:
            print(f"::endgroup::")

    # parallelly run
    async def run(k: str, cmd: str):
        p = await asyncio.create_subprocess_shell(
            " ".join(cmd),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await p.communicate()
        if gh_env is not None:
            print(f"::group::run {compile_mode} {opt_flag} {k} test")

        if stdout:
            print(stdout.decode())
        if p.returncode != 0 and stderr:
            print(stderr.decode())
        if gh_env is not None:
            print(f"::endgroup::")

        return p.returncode

    async def run_all():
        ts = []
        async with asyncio.TaskGroup() as g:
            for k, cmd in run_cmds.items():
                ts.append(g.create_task(run(k, cmd)))
        return reduce(lambda acc, c: acc or c, map(lambda t: t.result(), ts), exit_code)

    exit_code = asyncio.run(run_all())
    exit(exit_code)


if __name__ == "__main__":
    cli()
